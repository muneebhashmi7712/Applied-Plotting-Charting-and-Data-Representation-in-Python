{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### run this for imports ######\n",
    "from IPython.core.debugger import set_trace\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from os.path import exists\n",
    "import datetime\n",
    "import time\n",
    "import io\n",
    "import sys\n",
    "import uuid\n",
    "import xlrd\n",
    "from deep_translator import GoogleTranslator\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.identity import ManagedIdentityCredential\n",
    "# from azure.keyvault.secrets import SecretClient\n",
    "from couchbase.cluster import Cluster, ClusterOptions, QueryOptions\n",
    "from couchbase_core.cluster import PasswordAuthenticator\n",
    "from couchbase.management.collections import CollectionSpec\n",
    "from couchbase.exceptions import (\n",
    "    CollectionAlreadyExistsException,\n",
    "    CollectionNotFoundException,\n",
    "    ScopeAlreadyExistsException,\n",
    "    ScopeNotFoundException)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ref_data(filename):\n",
    "    try:\n",
    "        data = {\n",
    "        'Table_1' : {'Customer Data List' : 'Takeuchi Japan','Make': 'Takeuchi', 'First column': 'Model', 'Parent Name': 'TAKEUCHI', },\n",
    "         }\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "        print('reference data saved to json file')\n",
    "    except Exception as e:\n",
    "        print(str(\"reference data save to json failed error: \"+e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get reference data from couchbase collection\n",
    "def get_ref_data_from_cb(cluster, bucket, scope_name, collection_name, ref_id):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        \n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "\n",
    "        #result = collection.get('takeuchi_ref_pod')\n",
    "        result = collection.get(ref_id[0])\n",
    "        ref_pod = pd.read_json(result.content)\n",
    "        \n",
    "        #result = collection.get('takeuchi_ref_pol')\n",
    "        result = collection.get(ref_id[1])\n",
    "        ref_pol = pd.read_json(result.content)\n",
    "\n",
    "        \n",
    "        # result = collection.get('takeuchi_ref_data')\n",
    "        result = collection.get(ref_id[2])\n",
    "        ref_data = result.content\n",
    "\n",
    "        return ref_pod, ref_pol, ref_data\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_Azure_Blob_into_dataframe(location,sheetname,connect_str,container_name,sep_='\\t'):\n",
    "    try:            \n",
    "        # Create the BlobServiceClient object which will be used to create a container client\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=location)\n",
    "        with io.BytesIO() as input_blob:\n",
    "            #print(input_blob)\n",
    "            blob_client.download_blob().download_to_stream(input_blob)\n",
    "            input_blob.seek(0)\n",
    "            if pd.isna(sheetname):\n",
    "                df = pd.read_excel(input_blob, engine='openpyxl', header=None)\n",
    "            else:\n",
    "                df = pd.read_excel(input_blob,sheet_name=sheetname, engine='openpyxl', header=None)\n",
    "            \n",
    "            print(datetime.datetime.today(), \"DONE Reading From Azure Blob: \" + location)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(datetime.datetime.today(), e)\n",
    "        print(datetime.datetime.today(), 'Failed to Read Blob File')\n",
    "        sys.exit()\n",
    "    finally:\n",
    "        print(datetime.datetime.today(), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_local(file, sheetname):\n",
    "    try:\n",
    "        if pd.isna(sheetname):\n",
    "            df = pd.read_excel(file, engine='openpyxl', header=None)\n",
    "        else:\n",
    "            df = pd.read_excel(file,sheet_name=sheetname, engine='openpyxl', header=None)\n",
    "\n",
    "        print(datetime.datetime.today(), \"DONE Reading From local: \" + file)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(datetime.datetime.today(), e)\n",
    "        print(datetime.datetime.today(), 'Failed to Read from local')\n",
    "        sys.exit()\n",
    "    finally:\n",
    "        print(datetime.datetime.today(), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the row and starting column of the headers. These are used as reference points for raw data processing.\n",
    "def get_header_idx(df, ref_data):\n",
    "    try:    \n",
    "        first_column = ref_data.get('Table_1', {}).get('First column')\n",
    "        result = df.isin([first_column])\n",
    "        seriesObj = result.any()\n",
    "        header_col_idx = list(seriesObj[seriesObj == True].index)[0]\n",
    "        header_row_idx = list(result[header_col_idx][result[header_col_idx] == True].index)[0]\n",
    "        return(header_row_idx, header_col_idx)   \n",
    "    except Exception as e:\n",
    "        print(\"First column \\'\"+first_column+\"\\' not found\")\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_header(df, src='japanese'):\n",
    "\n",
    "    translator = GoogleTranslator(source=src, target='en')\n",
    "    new_cols = list(map(lambda x: translator.translate(x), df.columns))\n",
    "    uniform_cols = list(map(lambda x: x.lower().replace(\"\\n\", \" \").replace(\" \", \"_\"), new_cols))\n",
    "    df.set_axis(uniform_cols, axis='columns', inplace=True)\n",
    "#     cols = []\n",
    "#     count = 1\n",
    "#     for column in df.columns:\n",
    "#         if column == 'm3':\n",
    "#             cols.append(f'm3_{count}')\n",
    "#             count+=1\n",
    "#             continue\n",
    "#         cols.append(column)\n",
    "#     df.columns = cols\n",
    "\n",
    "#     new_cols = list(map(lambda x: df[x][0] if pd.isna(x) else x, df.columns))\n",
    "#     uniform_cols = list(map(lambda x: x.lower().replace(\" \", \"_\"), new_cols))\n",
    "#     df.set_axis(uniform_cols, axis='columns', inplace=True)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, ref_pol, ref_pod):\n",
    "\n",
    "    df = df[['model', 'unit', 'installation_date', 'destination_(pol)', 'note_(unloading)']]#, 'voy.', 'm3_2']]    \n",
    "    df['Flags'] = np.empty((len(df), 0)).tolist()    \n",
    "    df[\"qty\"] = [1]*len(df)\n",
    "    df = pd.merge(df, ref_pol[[\"Parsing data\", \"UNCODE\"]], left_on=\"destination_(pol)\", right_on=\"Parsing data\", how=\"left\")\n",
    "    df.rename(columns={\"UNCODE\": \"UNPOL\"}, inplace=True)\n",
    "    df = pd.merge(df, ref_pod[[\"Parsing data\", \"UNCODE\"]], left_on=\"note_(unloading)\", right_on=\"Parsing data\", how=\"left\")\n",
    "    df.rename(columns={\"UNCODE\": \"UNPOD\"}, inplace=True)\n",
    "    df = df[['model', 'unit', 'installation_date', 'destination_(pol)', 'note_(unloading)', 'qty', 'Flags', 'UNPOL', 'UNPOD']] #,'voy.', 'm3_2']]\n",
    "    df['installation_date'] = df['installation_date'].astype(str).apply(lambda x : np.nan if x=='NaT' else x)\n",
    "    df['installation_date'] = list(map(lambda x: fix_excel_date_format(x), df['installation_date']))\n",
    "       \n",
    "    return df\n",
    "    \n",
    "def fix_excel_date_format(x):\n",
    "    \n",
    "    try:\n",
    "        y = datetime.datetime.strptime(x, \"%Y-%m-%d  %H:%M:%S\")\n",
    "    except:\n",
    "#         print(x)\n",
    "        y = xlrd.xldate_as_datetime(int(x), 0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flags(df):\n",
    "    \n",
    "    try:\n",
    "#         null_model_idx = np.where(df['model'].isnull().values)[0].tolist()\n",
    "#         for row in null_model_idx:\n",
    "#             df.loc[row, 'Flags'].extend([{\"column\": \"Model\", \"error\": \"Null Value\"}])\n",
    "\n",
    "        \n",
    "        null_pol_idx = np.where(df['destination_(pol)'].isnull().values)[0].tolist()\n",
    "        for row in null_pol_idx:\n",
    "            df.loc[row, 'Flags'].append({\"column\": \"POL\", \"error\": \"Null Value in POL\"})\n",
    "#             df.loc[row, 'Flags'].append({\"column\": \"Trade\", \"error\": \"Null value in POL\"})\n",
    "        \n",
    "        missing_unpol_idx = np.where(df['UNPOL'].isnull().values)[0].tolist()\n",
    "        for row in missing_unpol_idx:\n",
    "            df.loc[row, 'Flags'].append({\"column\": \"POL\", \"error\": \"UN locode mapping not found\"})\n",
    "            df.loc[row,'UNPOL'] = str(df.loc[row, 'destination_(pol)'])\n",
    "#             df.loc[row, 'Flags'].append({\"column\": \"Trade\", \"error\": \"UNCODE not found for POL\"})\n",
    "            \n",
    "            \n",
    "        null_pod_idx = np.where(df[\"note_(unloading)\"].isnull().values)[0].tolist()\n",
    "        for row in null_pod_idx:\n",
    "            df.loc[row, 'Flags'].append({\"column\": \"POD\", \"error\": \"Null Value in POD\"})\n",
    "#             df.loc[row, 'Flags'].append({\"column\": \"Trade\", \"error\": \"Null value in POL\"})\n",
    "        \n",
    "        missing_unpod_idx = np.where(df['UNPOD'].isnull().values)[0].tolist()\n",
    "        for row in missing_unpod_idx:\n",
    "            df.loc[row, 'Flags'].append({\"column\": \"POD\", \"error\": \"UN locode mapping not found\"})\n",
    "            df.loc[row,'UNPOD'] = str(df.loc[row, \"note_(unloading)\"])\n",
    "#             df.loc[row, 'Flags'].append({\"column\": \"Trade\", \"error\": \"UNCODE not found for POD\"})\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for takeuchi period is identified by japanese month name in the filename\n",
    "def period_identification(filename):\n",
    "    \n",
    "    period = str(datetime.datetime.now().strftime(\"%Y\")+\"-\" + re.search(r'[0-9]+月', filename).group(0).split('월')[0])\n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get Trade, Sub-region, Cargo segment, CBM, SQM, Stow SQM, terminals,cargo ready date, cargo description,cargo type,equipment needed    columns using lookup into master data\n",
    "\n",
    "def get_master_cols(df, cluster, bucket, bucket_name, scope_name, trade_collection_name, model_collection_name, customer_collection_name, avg_dim_collection_name, stow_collection_name, terminal_collection_name, cargo_date_collection_name, equipment_collection_name):\n",
    "    \n",
    "    try:\n",
    "        parent = 'TAKEUCHI'\n",
    "#         make = 'HYUNDAI CONSTRUCTION EQUIPMENT'\n",
    "        trade_collection = bucket.scope(scope_name).collection(trade_collection_name)\n",
    "        \n",
    "        trade = list(map(lambda pol, pod: str(lookup_region(trade_collection, pol)+\"-\"+lookup_region(trade_collection, pod)), df['UNPOL'], df['UNPOD']))\n",
    "        subregion = list(map(lambda pol, pod: str(lookup_subregion(trade_collection, pol)+\"/\"+lookup_subregion(trade_collection, pod)), df['UNPOL'], df['UNPOD']))\n",
    "        \n",
    "        df['Trade'] = trade\n",
    "        df['Sub-region'] = subregion\n",
    "        \n",
    "       \n",
    "        # query model master data table\n",
    "        query_str = str(\"SELECT * from `\" + bucket_name+\"`.`\"+scope_name+\"`.`\"+model_collection_name+\"` WHERE `parent_name` LIKE $parent\")\n",
    "        result = cluster.query(query_str, QueryOptions(named_parameters={'parent': parent}))\n",
    "        results = [row.get(model_collection_name) for row in result]\n",
    "        \n",
    "        # Convert the results into a Pandas Dataframe\n",
    "        model_master_df = pd.DataFrame(results)\n",
    "        model_master_df = model_master_df.reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "        #query default segment from master data\n",
    "        query_str2 = str(\"SELECT default_cargo_segment FROM `\"+ bucket_name+\"`.`\"+scope_name+\"`.`\"+customer_collection_name+\"` WHERE parent LIKE $parent\")\n",
    "        \n",
    "        query_result = cluster.query(query_str2, QueryOptions(named_parameters={'parent': parent}))\n",
    "        \n",
    "        for row in query_result:\n",
    "            # print(row)\n",
    "            default_segment =  row.get(\"default_cargo_segment\")\n",
    "        #print(default_segment)\n",
    "        \n",
    "        default_segment = 'High and Heavy'\n",
    "        # print('\\n\\nthis is the cargo segment\\n\\n', default_segment)\n",
    "       \n",
    "        #query avg dimensions master data\n",
    "        query_str = str(\"SELECT * from `\" + bucket_name+\"`.`\"+scope_name+\"`.`\"+avg_dim_collection_name+\"` WHERE `customer` LIKE $parent\")\n",
    "        result = cluster.query(query_str, QueryOptions(named_parameters={'parent': parent}))\n",
    "        results = [row.get(avg_dim_collection_name) for row in result]\n",
    "        \n",
    "        # Convert the results into a Pandas Dataframe\n",
    "        avg_dim_df = pd.DataFrame(results)\n",
    "        \n",
    "        #query stow factors master data\n",
    "        query_str = str(\"SELECT * from `\" + bucket_name+\"`.`\"+scope_name+\"`.`\"+stow_collection_name+\"` WHERE `customer` LIKE $parent\")\n",
    "        result = cluster.query(query_str, QueryOptions(named_parameters={'parent': parent}))\n",
    "        results = [row.get(stow_collection_name) for row in result]\n",
    "        \n",
    "        # Convert the results into a Pandas Dataframe\n",
    "        stow_df = pd.DataFrame(results)\n",
    "        \n",
    "        #query stow factors master data if stow factors available only for a trade\n",
    "        query_str = str(\"SELECT * from `\" + bucket_name+\"`.`\"+scope_name+\"`.`\"+stow_collection_name+\"` WHERE `customer` LIKE $parent\")\n",
    "        result = cluster.query(query_str, QueryOptions(named_parameters={'parent': \"%nan%\"}))\n",
    "        results = [row.get(stow_collection_name) for row in result]\n",
    "        \n",
    "        # Convert the results into a Pandas Dataframe\n",
    "        stow_default = pd.DataFrame(results)\n",
    "        \n",
    "        #query terminal master data\n",
    "        query_str = str(\"SELECT * from `\" + bucket_name+\"`.`\"+scope_name+\"`.`\"+terminal_collection_name+\"` WHERE `customer` LIKE $parent\")\n",
    "        result = cluster.query(query_str, QueryOptions(named_parameters={'parent': parent}))\n",
    "        results = [row.get(terminal_collection_name) for row in result]\n",
    "        \n",
    "        # Convert the results into a Pandas Dataframe\n",
    "        terminal_df = pd.DataFrame(results)\n",
    "        \n",
    "        #query cargo ready date master data\n",
    "        query_str = str(\"SELECT * from `\" + bucket_name+\"`.`\"+scope_name+\"`.`\"+cargo_date_collection_name+\"` WHERE `parent_name` LIKE $parent\")\n",
    "        result = cluster.query(query_str, QueryOptions(named_parameters={'parent': parent}))\n",
    "        results = [row.get(cargo_date_collection_name) for row in result]\n",
    "        \n",
    "        # Convert the results into a Pandas Dataframe\n",
    "        cargo_date_df = pd.DataFrame(results)\n",
    "        \n",
    "        #query equipment needed master data\n",
    "        query_str = str(\"SELECT * from `\" + bucket_name+\"`.`\"+scope_name+\"`.`\"+equipment_collection_name+\"` WHERE meta().id IS NOT MISSING\")\n",
    "        result = cluster.query(query_str)\n",
    "        results = [row.get(equipment_collection_name) for row in result]\n",
    "        \n",
    "        # Convert the results into a Pandas Dataframe\n",
    "        equipment_df = pd.DataFrame(results)\n",
    "        \n",
    "        \n",
    "        #rename columns\n",
    "        model_master_df.rename(columns={'parent_uuid': 'Parent UUID', 'parent_name': 'Parent Name', 'make': 'Make', 'model_name': 'Model Name',\n",
    "                                'model_code': 'Model Code', 'cargo_segment': 'Cargo Segment', 'cargo_sub_segment': 'Cargo Sub-Segment', 'length': 'Length(m)', 'width': 'Width(m)',\n",
    "                                'height': 'Height(m)', 'weight': 'Weight(kg)', 'cbm': 'CBM', 'sqm': 'SQM','aeu': 'AEU','rt': 'RT'\n",
    "                                }, inplace=True)\n",
    "        avg_dim_df.rename(columns={'parent_uuid': 'Parent UUID', 'customer': 'Customer', 'make': 'Make', 'trade': 'Trade',\n",
    "                                'sub_region': 'Sub-Region', 'cargo_segment': 'Cargo Segment', 'cargo_sub_segment': 'Cargo Sub-Segment', 'pol': 'POL', 'pod': 'POD',\n",
    "                                'avg_l': 'Avg L', 'avg_w': 'Avg W', 'avg_h': 'Avg H', 'l_factor': 'L factor','w_factor': 'W factor','avg_cbm': 'Avg CBM', 'avg_sqm': 'Avg SQM', \n",
    "                                'avg_aeu': 'Avg AEU', 'avg_rt': 'Avg RT'}, inplace=True)\n",
    "        stow_df.rename(columns={'parent_uuid': 'Parent UUID', 'customer': 'Customer', 'make': 'Make', 'trade': 'Trade',\n",
    "                                'pol': 'POL', 'pod': 'POD', 'cargo_segment': 'Cargo Segment', 'cargo_sub_segment': 'Cargo Sub-Segment',\n",
    "                                'model_code': 'Model Code', 'model_name': 'Model Name', 'l_factor': 'L factor','w_factor': 'W factor'\n",
    "                                }, inplace=True)\n",
    "        terminal_df.rename(columns={'parent_uuid': 'Parent UUID', 'customer': 'customer', 'make': 'make', 'port': 'port',\n",
    "                                'default_terminal': 'default_terminal', 'pol_pod': 'pol/pod'\n",
    "                                }, inplace=True)\n",
    "        cargo_date_df.rename(columns={'parent_uuid': 'Parent UUID', 'parent_name': 'Parent Name', 'make': 'Make', 'model_code': 'Model Code',\n",
    "                                'trade': 'Trade', 'factory_code':'Factory Code', 'pol': 'POL', 'pod': 'POD', 'model': 'Model',\n",
    "                                'main_segment':'Main segment', 'further_segments': 'Further segments', 'lead_time_factory_to_pol': 'Lead Time Factory to POL',\n",
    "                                'lead_time_receiving_activity': 'Lead Time Receiving Activity',    \n",
    "                                }, inplace=True)\n",
    "        equipment_df.rename(columns={'trade': 'Trade', 'cargo_segment': 'Cargo Segment', 'length': 'L', 'width': 'W', 'height': 'H',\n",
    "                                'weight': 'Weight'\n",
    "                                }, inplace=True)\n",
    "        \n",
    "        \n",
    "        model_master_df['Cargo Type'] = 'Auto ≤ 1.95m'\n",
    "        \n",
    "        \n",
    "        ## using dataframe join technique for performance gains\n",
    "        \n",
    "        # left-join model master data with forecast data\n",
    "        joined_df = pd.merge(df, model_master_df[[\"Model Code\", \"Cargo Segment\", \"CBM\", \"SQM\",\"AEU\", \"RT\", \"Length(m)\", \"Width(m)\",\"Height(m)\", \"Weight(kg)\", \"Cargo Type\"]], left_on=\"model\", right_on=\"Model Code\", how=\"left\")\n",
    "        \n",
    "        # get cargo segment values\n",
    "        joined_df['Cargo Segment'].fillna(default_segment, inplace=True)\n",
    "        \n",
    "        joined_df.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        # left-join avg dimensions master data with forecast data\n",
    "        joined_df = pd.merge(joined_df, avg_dim_df[[\"Trade\",\"POL\", \"POD\", \"Cargo Segment\", \"Avg CBM\", \"Avg SQM\", \"Avg AEU\", \"Avg RT\"]], left_on=['Trade','UNPOL','UNPOD','Cargo Segment']  , right_on=[\"Trade\",\"POL\", \"POD\", \"Cargo Segment\"], how=\"left\")\n",
    "        joined_df.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "            \n",
    "        if pd.isna(joined_df[\"Avg CBM\"]).any():\n",
    "            joined_df = pd.merge(joined_df, avg_dim_df[[\"POL\", \"Cargo Segment\", \"Avg CBM\", \"Avg SQM\",\"Avg AEU\", \"Avg RT\"]], left_on=['UNPOL','Cargo Segment']  , right_on=[\"POL\", \"Cargo Segment\"], how=\"left\")\n",
    "            joined_df.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "       \n",
    "            joined_df['Avg CBM_x'].fillna(joined_df['Avg CBM_y'], inplace=True)\n",
    "            joined_df['Avg CBM_x'].fillna(0, inplace=True)\n",
    "            joined_df['Avg SQM_x'].fillna(joined_df['Avg SQM_y'], inplace=True)\n",
    "            joined_df['Avg SQM_x'].fillna(0, inplace=True)\n",
    "            joined_df['Avg AEU_x'].fillna(joined_df['Avg AEU_y'], inplace=True)\n",
    "            joined_df['Avg AEU_x'].fillna(0, inplace=True)\n",
    "            joined_df['Avg RT_x'].fillna(joined_df['Avg RT_y'], inplace=True)\n",
    "            joined_df['Avg RT_x'].fillna(0, inplace=True)\n",
    "        \n",
    "        \n",
    "        # get fill dimensions factor with average factor when it is missing from model master data\n",
    "            joined_df['CBM'].fillna(joined_df['Avg CBM_x'], inplace=True)#.fillna(0, inplace=True)\n",
    "            joined_df['SQM'].fillna(joined_df['Avg SQM_x'], inplace=True)#.fillna(0, inplace=True)\n",
    "            joined_df['AEU'].fillna(joined_df['Avg AEU_x'], inplace=True)\n",
    "            joined_df['RT'].fillna(joined_df['Avg RT_x'], inplace=True)\n",
    "        else:\n",
    "            joined_df['CBM'].fillna(joined_df['Avg CBM'], inplace=True)#.fillna(0, inplace=True)\n",
    "            joined_df['SQM'].fillna(joined_df['Avg SQM'], inplace=True)#.fillna(0, inplace=True)\n",
    "            joined_df['AEU'].fillna(joined_df['Avg AEU'], inplace=True)\n",
    "            joined_df['RT'].fillna(joined_df['Avg RT'], inplace=True)\n",
    "        \n",
    "       \n",
    "        # cbm/sqm/aeu/rt\n",
    "        \n",
    "        #calculate dimensions for each row (quantity x factor)\n",
    "        joined_df['Cubic meters final'] = list(map(lambda cbm, qty: round(int(qty)*float(cbm), 3), joined_df.CBM, joined_df['qty']))\n",
    "        joined_df['Square meters final'] = list(map(lambda sqm, qty: round(int(qty)*float(sqm), 3), joined_df.SQM, joined_df['qty']))\n",
    "        joined_df['AEU final'] = list(map(lambda aeu, qty: round(int(qty)*float(aeu), 3), joined_df.AEU, joined_df['qty']))\n",
    "        joined_df['RT final'] = list(map(lambda rt, qty: round(int(qty)*float(rt), 3), joined_df.RT, joined_df['qty']))\n",
    "          \n",
    "        \n",
    "      \n",
    "        # left-join stow factors master data with forecast data\n",
    "        joined_df = pd.merge(joined_df, stow_df[[\"Cargo Segment\",\"Trade\",\"L factor\", \"W factor\"]], left_on=[\"Cargo Segment\",\"Trade\"], right_on=[\"Cargo Segment\",\"Trade\"], how=\"left\")\n",
    "        \n",
    "#         #print(joined_df[\"Length(m)\"])\n",
    "        \n",
    "#         joined_df[\"length_(mm)\"].fillna(joined_df[\"Length(m)\"], inplace=True)\n",
    "#         joined_df[\"length_(mm)\"].fillna(0, inplace=True)\n",
    "#         joined_df[\"width_(mm)\"].fillna(joined_df[\"Width(m)\"], inplace=True)\n",
    "#         joined_df[\"width_(mm)\"].fillna(0, inplace=True)\n",
    "#         joined_df[\"height_(mm)\"].fillna(joined_df[\"Height(m)\"], inplace=True)\n",
    "#         joined_df[\"height_(mm)\"].fillna(0, inplace=True)\n",
    "#         joined_df[\"dimension\"].fillna(joined_df[\"Weight(kg)\"], inplace=True)\n",
    "#         joined_df[\"dimension\"].fillna(0, inplace=True)\n",
    "        \n",
    "        if pd.isna(joined_df[\"L factor\"]).any() and len(stow_default)>1:\n",
    "            joined_df = pd.merge(joined_df, stow_default[[\"Trade\",\"L factor\", \"W factor\"]], left_on=\"model\", right_on=\"Model Code\", how=\"left\")\n",
    "            joined_df['L factor_x'].fillna(joined_df['L factor_y'], inplace=True)\n",
    "            joined_df['L factor_x'].fillna(0, inplace=True)\n",
    "            joined_df['W factor_x'].fillna(joined_df['W factor_y'], inplace=True)\n",
    "            joined_df['W factor_x'].fillna(0, inplace=True)\n",
    "            \n",
    "            \n",
    "            joined_df['Stow'] = list(map(lambda length, width, l_factor, w_factor, qty: round(int(qty)*(float(l_factor)+float(length)) * (float(w_factor)+float(width)), 3), joined_df[\"Length(m)\"], joined_df[\"Width(m)\"], joined_df[\"L factor_x\"],joined_df[\"W factor_x\"],  joined_df['qty'] ))\n",
    "        else:\n",
    "            joined_df['Stow'] = list(map(lambda length, width, l_factor, w_factor, qty: round(int(qty)*(float(l_factor)+float(length)) * (float(w_factor)+float(width)), 3), joined_df[\"Length(m)\"], joined_df[\"Width(m)\"], joined_df[\"L factor\"],joined_df[\"W factor\"],  joined_df['qty'] ))\n",
    "        \n",
    "                    \n",
    "        # left-join terminal master data with forecast data\n",
    "        joined_df = pd.merge(joined_df, terminal_df[[\"port\", \"default_terminal\"]], left_on=[\"UNPOL\"], right_on=[\"port\"], how=\"left\")\n",
    "        joined_df = joined_df.rename(columns={\"default_terminal\": \"POL_terminal\"})\n",
    "        \n",
    "        joined_df = pd.merge(joined_df, terminal_df[[\"port\", \"default_terminal\"]], left_on=[\"UNPOD\"], right_on=[\"port\"], how=\"left\")\n",
    "        joined_df = joined_df.rename(columns={\"default_terminal\": \"POD_terminal\"})\n",
    "        \n",
    "        # get cargo ready date\n",
    "        if len(cargo_date_df)>0:\n",
    "            joined_df = pd.merge(joined_df, cargo_date_df[[\"Trade\", \"Lead Time Receiving Activity\"]], left_on=[\"Trade\"], right_on=[\"Trade\"], how=\"left\")\n",
    "            joined_df = joined_df.rename(columns={\"Lead Time Receiving Activity\": \"Lead Time_1\"})\n",
    "\n",
    "            joined_df = pd.merge(joined_df, cargo_date_df[[\"Trade\", \"Lead Time Receiving Activity\"]], left_on=[\"Trade\"], right_on=[\"Trade\"], how=\"left\")\n",
    "            joined_df = joined_df.rename(columns={\"Lead Time Receiving Activity\": \"Lead Time_2\"})\n",
    "\n",
    "            joined_df[\"Lead Time_1\"] = list(map(lambda x, y: y if pd.isna(x) else x, joined_df[\"Lead Time_1\"], joined_df[\"Lead Time_2\"]))\n",
    "\n",
    "            joined_df[\"Lead Time_1\"] = list(map(lambda x: 0 if pd.isna(x) else x, joined_df[\"Lead Time_1\"]))\n",
    "        else:\n",
    "            joined_df[\"Lead Time_1\"] = [0]*len(df)\n",
    "        \n",
    "        joined_df[\"cargo_ready_date\"] = list(map(lambda x, y: x + datetime.timedelta(days=int(y)), joined_df['installation_date'], joined_df[\"Lead Time_1\"]))\n",
    "\n",
    "        joined_df = pd.merge(joined_df, equipment_df[[\"L\", \"W\", \"H\", \"Weight\", \"Trade\", \"Cargo Segment\"]], left_on=[\"Trade\", \"Cargo Segment\"], right_on=[\"Trade\", \"Cargo Segment\"], how=\"left\") \n",
    "        \n",
    "        if pd.isna(joined_df[\"L\"]).all():\n",
    "            \n",
    "            joined_df = joined_df.drop([\"L\", \"W\", \"H\", \"Weight\",], axis=1)\n",
    "            joined_df[\"equipment_needed\"] = [\"N\"]*len(joined_df)\n",
    "        else:\n",
    "            joined_df[\"equipment_needed\"] = list(map(lambda l,w,h,dimension,L,W,H,Weight: \"Y\" if L<l or W<w or H<h or Weight<dimension else \"N\",joined_df[\"Length(m)\"], joined_df[\"Width(m)\"], joined_df[\"Height(m)\"], joined_df[\"Weight(kg)\"],  joined_df[\"L\"], joined_df[\"W\"], joined_df[\"H\"], joined_df[\"Weight\"]  ))\n",
    "        \n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lookup region to calculate trade\n",
    "def lookup_region(collection, port):\n",
    "    region = np.nan\n",
    "    try:\n",
    "        rv = collection.get(port)\n",
    "        region = rv.content.get('region')\n",
    "\n",
    "        if pd.isna(region):\n",
    "            region == \"\"\n",
    "    except:\n",
    "        region == \"\"\n",
    "    \n",
    "    return str(region)\n",
    "\n",
    "# function to lookup subregion\n",
    "def lookup_subregion(collection, port):\n",
    "    subregion = np.nan\n",
    "    try:\n",
    "        rv = collection.get(port)\n",
    "        subregion = rv.content.get('final_sub_region')\n",
    "        if pd.isna(subregion):\n",
    "            subregion = \"\"\n",
    "    except:\n",
    "        subregion = \"\"\n",
    "    \n",
    "    return str(subregion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantity_flags(df, cluster, bucket, cb_scope_name, forecast_coll_name):\n",
    "    \n",
    "    collection = bucket.scope(cb_scope_name).collection(forecast_coll_name)\n",
    "    \n",
    "    for idx in range(len(df)):\n",
    "        current_qty = int(df.loc[idx, 'Number of Units'])\n",
    "        try:\n",
    "#             key = str(df.loc[idx, 'Customer Name']+'-'+df.loc[idx, 'work_order'])\n",
    "            #str(df.loc[idx, 'Customer Name']+'-'+df.loc[idx, 'POL (Port of Loading)']+'-'+df.loc[idx, 'POD (Port Of Destination)']+'-'+df.loc[idx, 'Model']+'-'+df.loc[idx, 'Production date'])\n",
    "            key = str(df.loc[idx, 'Customer Name']+'-'+df.loc[idx, 'POL (Port of Loading)']+'-'+df.loc[idx, 'POD (Port Of Destination)']+'-'+df.loc[idx, 'Model']+'-'+df.loc[idx, 'cargo_ready_date'])\n",
    "\n",
    "            result = collection.get(key).content_as[dict]\n",
    "            prev_qty = int(result.get('Number of Units'))\n",
    "            if current_qty==0:\n",
    "                if prev_qty!=0:\n",
    "                    df.loc[idx, 'Flags'].append({\"column\": \"Number of Units\", \"error\": str(\"Quantity jump to 0 from \"+str(prev_qty))})\n",
    "                else:\n",
    "                    df.loc[idx,'Flags'].append({\"column\": \"Number of Units\", \"error\": \"Quantity is 0\"})\n",
    "            elif current_qty!=0:\n",
    "                if prev_qty<=100 and abs(current_qty-prev_qty)>=10:\n",
    "                    df.loc[idx, 'Flags'].append({\"column\": \"Number of Units\", \"error\": str(\"Quantity jump to \" +str(current_qty)+\" from \"+str(prev_qty))})\n",
    "                elif prev_qty>100 and abs(current_qty-prev_qty)>=(prev_qty*0.1):\n",
    "                    df.loc[idx, 'Flags'].append({\"column\": \"Number of Units\", \"error\": str(\"Quantity jump to \" +str(current_qty)+\" from \"+str(prev_qty))})\n",
    "\n",
    "        except:\n",
    "            if current_qty==0:\n",
    "                df.loc[idx,'Flags'].append({\"column\": \"Number of Units\", \"error\": \"Quantity is 0\"})\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save data to couchbase using key-value CRUD operations\n",
    "def save_to_cb(csv_path, flags,unit, cluster, bucket, cb_scope_name, forecast_coll_name, period_identifier, bucket_name, customer_name):\n",
    "    try:\n",
    "        \n",
    "        query_str = str(\"DELETE FROM `\"+ bucket_name+\"`.`\"+cb_scope_name+\"`.`\"+forecast_coll_name+\"` WHERE period_identifier LIKE $period_identifier AND `Customer Name` LIKE $customer RETURNING meta().id\")\n",
    "        query_result = cluster.query(query_str, QueryOptions(named_parameters={'period_identifier': period_identifier, 'customer': customer_name}))\n",
    "#         if query_result.rows():\n",
    "#             print(str(str(len(query_result.rows()))+\" rows deleted\"))\n",
    "        \n",
    "        \n",
    "        with open(csv_path) as csvfile:\n",
    "            lines = csvfile.readlines()\n",
    "        BYTES_PER_BATCH = 1024 * 256 # 256K\n",
    "\n",
    "        batches = []\n",
    "        cur_batch = {}\n",
    "        cur_size = 0\n",
    "        batches.append(cur_batch)\n",
    "        cols = lines[0].strip().split(',')\n",
    "        for i in range(1, len(lines)):\n",
    "            line = lines[i]\n",
    "            datastore = line.strip().split(',')\n",
    "\n",
    "            #building kv\n",
    "            value = {\n",
    "                cols[0]: datastore[0],\n",
    "                cols[1]: datastore[1],\n",
    "                cols[2]: datastore[2],\n",
    "                cols[3]: datastore[3],\n",
    "                cols[4]: datastore[4],\n",
    "                cols[5]: datastore[5],\n",
    "                cols[6]: datastore[6],\n",
    "                cols[7]: datastore[7],\n",
    "                cols[8]: datastore[8],\n",
    "                cols[9]: datastore[9],\n",
    "                cols[10]: datastore[10],\n",
    "                cols[11]: datastore[11],\n",
    "                cols[12]: datastore[12],\n",
    "                cols[13]: datastore[13],\n",
    "                cols[14]: datastore[14],\n",
    "                cols[15]: datastore[15],\n",
    "                cols[16]: datastore[16],\n",
    "                cols[17]: datastore[17],\n",
    "                cols[18]: datastore[18],\n",
    "                cols[19]: datastore[19],\n",
    "                cols[20]: datastore[20],\n",
    "                cols[21]: datastore[21],\n",
    "                cols[21]: datastore[21],\n",
    "                cols[21]: datastore[21],\n",
    "                cols[22]: datastore[22],\n",
    "                cols[23]: datastore[23],\n",
    "                cols[24]: datastore[24],\n",
    "                cols[25]: datastore[25],\n",
    "                'unit': unit[i-1],\n",
    "                'Flags': flags[i-1],\n",
    "                \n",
    "                #\n",
    "            }\n",
    "            key = str(datastore[0]+'-'+datastore[5]+'-'+datastore[6]+'-'+datastore[7]+'-'+datastore[8])\n",
    "            cur_batch[key] = value\n",
    "            cur_size += len(key) + len(value) + 24\n",
    "\n",
    "            if cur_size > BYTES_PER_BATCH:\n",
    "                print(\"cur_size > bytes_per_batch\")\n",
    "                cur_batch = {}\n",
    "                batches.append(cur_batch)\n",
    "                cur_size = 0\n",
    "        print(\"Have {} batches\".format(len(batches)))\n",
    "         #collection = bucket.scope(cb_scope_name).collection(\"devops_test_collection\") #for testing only\n",
    "        \n",
    "        collection = bucket.scope(cb_scope_name).collection(forecast_coll_name)\n",
    "        num_completed = 0\n",
    "        while batches:\n",
    "            batch = batches[-1]\n",
    "            try:\n",
    "                collection.upsert_multi(batch)\n",
    "                num_completed+=len(batch)\n",
    "                batches.pop()\n",
    "            except Exception as e:\n",
    "                raise(e)\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add transaction for processing in database\n",
    "def add_file_transaction(bucket_name, master_scope_name, parent_collection_name, path, filename, process_time, transaction_collection_name,cluster, bucket, scope_name, ref_data, customer_name, parent, is_reprocessed=False):\n",
    "    \n",
    "   \n",
    "    try:\n",
    "    \n",
    "#         parent = ref_data.get('Table_1', {}).get('Parent Name')\n",
    "#         customer_name = ref_data.get('Table_1', {}).get('Customer Data List')\n",
    "\n",
    "        query_str = str(\"SELECT meta().id FROM `\"+ bucket_name+\"`.`\"+master_scope_name+\"`.`\"+parent_collection_name+\"` WHERE parent_name LIKE $parent\")\n",
    "        query_result = cluster.query(query_str, QueryOptions(named_parameters={'parent': parent}))\n",
    "        for row in query_result:\n",
    "            p_uuid = row.get('id')  \n",
    "\n",
    "        collection = bucket.scope(scope_name).collection(transaction_collection_name)\n",
    "\n",
    "\n",
    "        value = {\n",
    "            \"Parent uuid\": p_uuid,\n",
    "            \"Parent name\": parent,\n",
    "            \"Customer name\": customer_name,\n",
    "            \"filename\": filename,\n",
    "            \"path\": path,\n",
    "            \"Processing_time\": process_time,\n",
    "            \"is_reprocessed\" : is_reprocessed\n",
    "\n",
    "        }\n",
    "\n",
    "        key = uuid.uuid4().__str__()\n",
    "        result = collection.upsert(key, value)\n",
    "        cas = result.cas\n",
    "        print(str(cas))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(location_file=\"Original File_Takeuchi_7月 RORO WWOcean_20220622.xlsx\", sheetname=\"Jul'22\", is_reprocessed=False):\n",
    "    try:\n",
    "        \n",
    "        connection_str = \"couchbase://172.16.0.4, 172.16.0.5, 172.16.0.6\"\n",
    "        username = 'cbadmin'\n",
    "        password = '2021cbadmin!walwil'\n",
    "                \n",
    "        azure_connection_str = \"DefaultEndpointsProtocol=https;AccountName=analyticsblob1;AccountKey=euKVvGVA7mwuHPL7tXsiKQM88R+G2xA1el2TdLBn/SqokFy1O3tyvMVT/qn6T+tzukY9vf+0fBbrKxpEGsTRYg==;EndpointSuffix=core.windows.net\"\n",
    "        \n",
    "        TENANT = 'cdffb2cf-9686-4fe1-ae64-24e2074f2a7e'\n",
    "        CLIENT_ID = '429bfcf6-a929-4dce-bd9a-a20259c19a72'\n",
    "        CLIENT_SECRET = 'ZmZ7Q~xa_nI4Vd52AX~N7mX7nH5hZgAoSlmOv'\n",
    "        \n",
    "        vault_name = \"s-shippingtransformation\"\n",
    "        VAULT_URL= str('https://'+vault_name+'.vault.azure.net/')\n",
    "        azure_container_name = \"shippingtransformation\"\n",
    "        \n",
    "        path = \"TAKEUCHI/\"\n",
    "        cb_bucket_name = \"onebridge\"\n",
    "        cb_scope_name = \"useful_data\"\n",
    "        cb_ref_collection_name = \"customer_reference_data\"\n",
    "        forecast_coll_name = \"customer_forecast\"\n",
    "        ref_id = [\"takeuchi_ref_pod\", \"takeuchi_ref_pol\", \"takeuchi_ref_data\"]\n",
    "        trade_mapping_coll_name = \"port_codes\"\n",
    "        master_scope_name = \"master_data_vtwo\"\n",
    "        model_coll_name = \"customer_model_list\"\n",
    "        customer_coll_name = \"customer_name_list\"\n",
    "        parent_coll_name = \"parent_name_list\"\n",
    "        transaction_coll_name = \"file_processing_history\"\n",
    "        avg_dim_coll_name = \"average_dimensions\"\n",
    "        stow_factors_coll_name = \"stow_factors\"\n",
    "        terminal_coll_name = \"port_terminals\"\n",
    "        cargo_date_coll_name = \"cargo_ready_date\"\n",
    "        equipment_coll_name = \"equipment_needed\"\n",
    "        \n",
    "#         cluster = Cluster(connection_str, ClusterOptions(PasswordAuthenticator(username, password)))\n",
    "#         bucket = cluster.bucket(cb_bucket_name)\n",
    "#         #print(bucket)\n",
    "        cluster = Cluster(\"couchbase://10.182.28.4, 10.182.28.5, 10.182.28.6\", ClusterOptions(PasswordAuthenticator('onebridge_dev_user', '@bom4Ee^O%eGh2Ff96')))\n",
    "        bucket = cluster.bucket('onebridge')\n",
    "        \n",
    "        #getting reference data from couchbase or json\n",
    "                \n",
    "        try:\n",
    "            pod_df, pol_df, ref_data = get_ref_data_from_cb(cluster, bucket, cb_scope_name, cb_ref_collection_name, ref_id)\n",
    "            #print(ref_data)\n",
    "        except:\n",
    "            print(\"ref data not found on database\")\n",
    "            filename = 'hce_ref_data.json'\n",
    "            file_exists = exists(filename)\n",
    "            if (not file_exists):\n",
    "                save_ref_data(filename)\n",
    "            with open(filename) as infile:\n",
    "                ref_data = json.load(infile)\n",
    "               \n",
    "        \n",
    "        parent = ref_data.get('Table_1', {}).get('Parent Name')\n",
    "        make = ref_data.get('Table_1', {}).get('Make')\n",
    "        customer_name = ref_data.get('Table_1', {}).get('Customer Data List')\n",
    "        print('\\n\\nref_data: \\n', ref_data)\n",
    "        print('\\n\\nparent: \\n', parent)\n",
    "        print('\\n\\nmake: \\n', make)\n",
    "        print('\\n\\ncustomer_name: \\n', customer_name)\n",
    "        #df = pd.read_excel(\"6.16 유럽 물량.xlsx\", engine='openpyxl', header=1)\n",
    "        period_identifier = period_identification(location_file)\n",
    "        \n",
    "        # raw_data = Read_Azure_Blob_into_dataframe(path+location_file,sheetname,azure_connection_str,azure_container_name)\n",
    "        raw_data = read_from_local(location_file,sheetname)\n",
    "        raw_data = raw_data.dropna(axis=1, how='all')\n",
    "        header_row, header_col = get_header_idx(raw_data, ref_data)\n",
    "        print('\\n\\nheader row: \\n', header_row)\n",
    "        print('\\n\\nheader col: \\n', header_col)\n",
    "        # set_trace()\n",
    "        \n",
    "        raw_df = pd.DataFrame(raw_data.values[header_row+1:], columns=raw_data.iloc[header_row])\n",
    "        raw_df = raw_df.dropna(how='all')  \n",
    "        print('\\n\\nraw df after slicing by header: \\n', raw_df)\n",
    "        print('\\nand the cols are: ', raw_df.columns)\n",
    "        # set_trace()\n",
    "        \n",
    "        raw_df = preprocess_header(raw_df)\n",
    "        print('\\n\\nraw df after preprocess header: \\n', raw_df)\n",
    "        print('\\nand the cols are: ', raw_df.columns)\n",
    "        df_clean = preprocess_df(raw_df, pol_df, pod_df)\n",
    "        print('\\n\\ndf_clean after preprocess_df\\n', df_clean)\n",
    "        print('\\nand the cols are: ', df_clean.columns)\n",
    "        df_clean = get_flags(df_clean)\n",
    "        df_clean = df_clean[['model', 'unit', 'installation_date', 'qty', 'Flags', 'UNPOL', 'UNPOD']] #, 'voy.', 'm3_2'\n",
    "        # set_trace()\n",
    "\n",
    "        \n",
    "#         df_clean = df_clean.fillna({\"m3_2\":0}).fillna(\"NA\")\n",
    "        cols = df_clean.columns.tolist()\n",
    "        \n",
    "        #removing all numeric attributes for groupby\n",
    "#         cols.remove('m3_2')\n",
    "        cols.remove('qty')\n",
    "        cols.remove('unit')\n",
    "        cols.remove('Flags')\n",
    "        \n",
    "        df_group = df_clean.groupby(cols, as_index=False).agg({'qty': 'sum', 'Flags': 'first','unit': lambda x: list(x)})#, 'm3_2': 'sum'\n",
    "        df_group.reset_index(drop=True)\n",
    "        # set_trace()\n",
    "        \n",
    "        df_group = df_group.replace(\"NA\", np.nan)\n",
    "        \n",
    "        joined_df = get_master_cols(df_group, cluster, bucket, cb_bucket_name, master_scope_name,  trade_mapping_coll_name, model_coll_name, customer_coll_name, avg_dim_coll_name, stow_factors_coll_name, terminal_coll_name, cargo_date_coll_name, equipment_coll_name)\n",
    "        # set_trace()\n",
    "        \n",
    "        joined_df['Customer Name'] = [customer_name]*len(joined_df)\n",
    "        joined_df['Parent Name'] = [parent]*len(joined_df)\n",
    "        joined_df['Make'] = [make]*len(joined_df)\n",
    "        final_df = joined_df[['Customer Name', 'Parent Name', 'Make', 'Trade', 'Cargo Segment','model', 'UNPOL', 'UNPOD', 'installation_date','POL_terminal','POD_terminal', 'qty', 'Cubic meters final', 'Square meters final', 'AEU final', 'RT final', 'Stow', 'cargo_ready_date', 'equipment_needed','Sub-region', 'unit', 'Flags','Length(m)','Width(m)','Height(m)','Weight(kg)']]#, 'voy.'\n",
    "        final_df.rename(columns={'Cargo Segment': 'Cargo segment','model': 'Model', 'UNPOL': 'POL (Port of Loading)','UNPOD': 'POD (Port Of Destination)', 'installation_date': 'planned_delivery_date_at_pol', 'qty': 'Number of Units',\n",
    "                                 'Cubic meters final':  'Cubic meters (CBM)', 'Square meters final': 'Square meters (SQM)', 'Stow': 'stow_sqm','AEU final': 'AEU', 'RT final': 'RT', 'voy.': 'target_voyage','Length(m)':'length_m','Width(m)':'width_m','Height(m)':'height_m','Weight(kg)':'weight_kg'}, inplace = True)\n",
    "                                 \n",
    "        final_df.replace(np.nan, '', regex=True)\n",
    "        final_df.replace('nan', '', regex=True)\n",
    "        \n",
    "        get_quantity_flags(final_df, cluster, bucket, cb_scope_name, forecast_coll_name)\n",
    "\n",
    "        final_df['period_identifier'] = [period_identifier]*len(final_df)\n",
    "        \n",
    "        final_df[\"planned_delivery_date_at_pol\"] = list(map(lambda x: x.strftime(\"%Y-%m-%d\"), final_df[\"planned_delivery_date_at_pol\"]))\n",
    "        final_df[\"cargo_ready_date\"] = list(map(lambda x: x.strftime(\"%Y-%m-%d\"), final_df[\"cargo_ready_date\"]))\n",
    "        \n",
    "        \n",
    "        final_df.fillna(\"\", inplace=True)\n",
    "        \n",
    "        datetime_utc = [str(datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\"))]*len(final_df)\n",
    "        \n",
    "        final_df['Process Datetime (UTC)'] = datetime_utc\n",
    "        \n",
    "\n",
    "\n",
    "        flags = final_df.Flags.values.tolist()\n",
    "        final_df.drop('Flags', inplace=True, axis=1)\n",
    "        \n",
    "        unit = final_df.unit.values.tolist()\n",
    "        final_df.drop('unit', inplace=True, axis=1)\n",
    "        \n",
    "        \n",
    "\n",
    "        final_df.to_csv(str('Mandatory_Data_Points-takeuchi.csv'), index=False)\n",
    "        print(\"File saved: \", datetime.datetime.now())\n",
    "\n",
    "        # save_to_cb(str('Mandatory_Data_Points-takeuchi.csv'), flags,unit, cluster, bucket, cb_scope_name, forecast_coll_name, period_identifier, cb_bucket_name, customer_name)\n",
    "        # print(\"Data saved to couchbase: \", datetime.datetime.now())\n",
    "\n",
    "        final_df['Flags'] = flags\n",
    "        final_df['unit'] = unit\n",
    "        final_df.to_csv(str('Mandatory_Data_Points-takeuchi.csv'), index=False)\n",
    "        print(\"File saved: \", datetime.datetime.now())\n",
    "        # add_file_transaction(cb_bucket_name, master_scope_name,  parent_coll_name, path, location_file, str(datetime_utc[0]), transaction_coll_name,cluster, bucket, cb_scope_name, ref_data, customer_name, parent, is_reprocessed)\n",
    "        # print(\"File processing history added to database: \", datetime.datetime.now())\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        return final_df\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14-01-2023 09:07:52  Start Main Function: \n",
      "\n",
      "\n",
      "ref_data: \n",
      " {'Table_1': {'Customer Data List': 'Takeuchi Japan', 'Make': 'Takeuchi', 'First column': 'Model', 'Parent Name': 'TAKEUCHI'}}\n",
      "\n",
      "\n",
      "parent: \n",
      " TAKEUCHI\n",
      "\n",
      "\n",
      "make: \n",
      " Takeuchi\n",
      "\n",
      "\n",
      "customer_name: \n",
      " Takeuchi Japan\n",
      "2023-01-14 09:07:52.519370 DONE Reading From local: Original File_Takeuchi_7月 RORO WWOcean_20220622.xlsx\n",
      "2023-01-14 09:07:52.519697 \n",
      "\n",
      "\n",
      "header row: \n",
      " 0\n",
      "\n",
      "\n",
      "header col: \n",
      " 0\n",
      "\n",
      "\n",
      "raw df after slicing by header: \n",
      " 0      Model         号機   得意先 PI\\n(00000xxxxx) 基本仕様    PID  CAB  CNP  \\\n",
      "0       TL6R  406004037    US           160619   us  16796  NaN    U   \n",
      "1       TL6R  406004038    US           160619   us  16796  NaN    U   \n",
      "2       TL6R  406004039    US           160619   us  16796  NaN    U   \n",
      "3       TL6R  406004046    US           160619   us  16796  NaN    U   \n",
      "4       TL6R  406004047    US           160620   us  16796  NaN    U   \n",
      "..       ...        ...   ...              ...  ...    ...  ...  ...   \n",
      "96   TB280FR  178502328   ドイツ           160204  exc  17445    E  NaN   \n",
      "97   TB280FR  178502329  フランス           159945  exc  13793    E  NaN   \n",
      "98   TB280FR  178502330  フランス           159946  exc  13793    E  NaN   \n",
      "99   TB280FR  178502335  フランス           159947  exc  13793    E  NaN   \n",
      "100  TB280FR  178502336  フランス           159948  exc  13793    E  NaN   \n",
      "\n",
      "0                   納期設定                Order  ...   Voy.       m3 Steel  \\\n",
      "0    2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   NaN   \n",
      "1    2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   NaN   \n",
      "2    2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   NaN   \n",
      "3    2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   NaN   \n",
      "4    2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   NaN   \n",
      "..                   ...                  ...  ...    ...      ...   ...   \n",
      "96                 44737                44713  ...  AT207      NaN   NaN   \n",
      "97                 44737                44713  ...  AT207      NaN   NaN   \n",
      "98                 44737                44713  ...  AT207      NaN   NaN   \n",
      "99                 44747                44713  ...  AT207      NaN   NaN   \n",
      "100                44747                44713  ...  AT207      NaN   NaN   \n",
      "\n",
      "0   Booking no.                                                変更点   地域   台数  \\\n",
      "0           NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米    1   \n",
      "1           NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米    1   \n",
      "2           NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米    1   \n",
      "3           NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米    1   \n",
      "4           NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米    1   \n",
      "..          ...                                                ...  ...  ...   \n",
      "96          NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN  NaN   \n",
      "97          NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN  NaN   \n",
      "98          NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN  NaN   \n",
      "99          NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN  NaN   \n",
      "100         NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN  NaN   \n",
      "\n",
      "0   北米直航\\n欧州直航\\nBAL T/S     m3 カテゴリ  \n",
      "0                  北米直航  8.244    A  \n",
      "1                  北米直航  8.244    A  \n",
      "2                  北米直航  8.244    A  \n",
      "3                  北米直航  8.244    A  \n",
      "4                  北米直航  8.244    A  \n",
      "..                  ...    ...  ...  \n",
      "96                  NaN    NaN  NaN  \n",
      "97                  NaN    NaN  NaN  \n",
      "98                  NaN    NaN  NaN  \n",
      "99                  NaN    NaN  NaN  \n",
      "100                 NaN    NaN  NaN  \n",
      "\n",
      "[101 rows x 36 columns]\n",
      "\n",
      "and the cols are:  Index(['Model', '号機', '得意先', 'PI\\n(00000xxxxx)', '基本仕様', 'PID', 'CAB', 'CNP',\n",
      "       '納期設定', 'Order', 'NOTE \\n(Order)', 'NOTE \\n（生産1）\\n仕様', 'NOTE \\n（生産2）',\n",
      "       'NOTE \\n（生産3）\\n同載', '完成指示日', '完成日', '出荷日', '搬入日', '搬入\\n場所', '搬入先(POL)',\n",
      "       'Invoice #', 'CY-Cut', 'ETD', 'ETA\\n6/20現在', '船名', 'NOTE \\n(荷卸）',\n",
      "       'Voy.', 'm3', 'Steel', 'Booking no.', '変更点', '地域', '台数',\n",
      "       '北米直航\\n欧州直航\\nBAL T/S', 'm3', 'カテゴリ'],\n",
      "      dtype='object', name=0)\n",
      "\n",
      "\n",
      "raw df after preprocess header: \n",
      "        model       unit customer pis_(00000xxxxx) basic_specifications   pids  \\\n",
      "0       TL6R  406004037       US           160619                   us  16796   \n",
      "1       TL6R  406004038       US           160619                   us  16796   \n",
      "2       TL6R  406004039       US           160619                   us  16796   \n",
      "3       TL6R  406004046       US           160619                   us  16796   \n",
      "4       TL6R  406004047       US           160620                   us  16796   \n",
      "..       ...        ...      ...              ...                  ...    ...   \n",
      "96   TB280FR  178502328      ドイツ           160204                  exc  17445   \n",
      "97   TB280FR  178502329     フランス           159945                  exc  13793   \n",
      "98   TB280FR  178502330     フランス           159946                  exc  13793   \n",
      "99   TB280FR  178502335     フランス           159947                  exc  13793   \n",
      "100  TB280FR  178502336     フランス           159948                  exc  13793   \n",
      "\n",
      "    cabs  cnp delivery_date_setting                order  ...   voy.       m3  \\\n",
      "0    NaN    U   2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   \n",
      "1    NaN    U   2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   \n",
      "2    NaN    U   2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   \n",
      "3    NaN    U   2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   \n",
      "4    NaN    U   2022-06-25 00:00:00  2022-06-01 00:00:00  ...  AF234  8.24445   \n",
      "..   ...  ...                   ...                  ...  ...    ...      ...   \n",
      "96     E  NaN                 44737                44713  ...  AT207      NaN   \n",
      "97     E  NaN                 44737                44713  ...  AT207      NaN   \n",
      "98     E  NaN                 44737                44713  ...  AT207      NaN   \n",
      "99     E  NaN                 44747                44713  ...  AT207      NaN   \n",
      "100    E  NaN                 44747                44713  ...  AT207      NaN   \n",
      "\n",
      "    steel booking_no.                                       change_point area  \\\n",
      "0     NaN         NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米   \n",
      "1     NaN         NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米   \n",
      "2     NaN         NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米   \n",
      "3     NaN         NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米   \n",
      "4     NaN         NaN            6/20 船名変更（TAMESIS AF238→TONSBERG AF234）   北米   \n",
      "..    ...         ...                                                ...  ...   \n",
      "96    NaN         NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN   \n",
      "97    NaN         NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN   \n",
      "98    NaN         NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN   \n",
      "99    NaN         NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN   \n",
      "100   NaN         NaN  6/15 船名変更（TITUS AE223→CARMEN AE223）　　 　　　　　6/2...  NaN   \n",
      "\n",
      "    number direct_to_north_america_europe_direct_bal_t/s     m3 category  \n",
      "0        1                                          北米直航  8.244        A  \n",
      "1        1                                          北米直航  8.244        A  \n",
      "2        1                                          北米直航  8.244        A  \n",
      "3        1                                          北米直航  8.244        A  \n",
      "4        1                                          北米直航  8.244        A  \n",
      "..     ...                                           ...    ...      ...  \n",
      "96     NaN                                           NaN    NaN      NaN  \n",
      "97     NaN                                           NaN    NaN      NaN  \n",
      "98     NaN                                           NaN    NaN      NaN  \n",
      "99     NaN                                           NaN    NaN      NaN  \n",
      "100    NaN                                           NaN    NaN      NaN  \n",
      "\n",
      "[101 rows x 36 columns]\n",
      "\n",
      "and the cols are:  Index(['model', 'unit', 'customer', 'pis_(00000xxxxx)', 'basic_specifications',\n",
      "       'pids', 'cabs', 'cnp', 'delivery_date_setting', 'order', 'note_(order)',\n",
      "       'note_(production_1)_specification', 'note_(production_2)',\n",
      "       'note_(production_3)_illustrated', 'completion_date', 'completion_date',\n",
      "       'ship_date', 'installation_date', 'carry-in_place', 'destination_(pol)',\n",
      "       'invoice_#', 'cy-cut', 'etds', 'eta_as_of_6/20', 'ship_name',\n",
      "       'note_(unloading)', 'voy.', 'm3', 'steel', 'booking_no.',\n",
      "       'change_point', 'area', 'number',\n",
      "       'direct_to_north_america_europe_direct_bal_t/s', 'm3', 'category'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "df_clean after preprocess_df\n",
      "        model       unit installation_date destination_(pol) note_(unloading)  \\\n",
      "0       TL6R  406004037        2022-06-23           NGY(WW)         SAVANNAH   \n",
      "1       TL6R  406004038        2022-06-23           NGY(WW)         SAVANNAH   \n",
      "2       TL6R  406004039        2022-06-23           NGY(WW)         SAVANNAH   \n",
      "3       TL6R  406004046        2022-06-27           NGY(WW)         SAVANNAH   \n",
      "4       TL6R  406004047        2022-06-27           NGY(WW)         SAVANNAH   \n",
      "..       ...        ...               ...               ...              ...   \n",
      "96   TB280FR  178502328        2022-06-23           NGY(WW)        ZEEBRUGGE   \n",
      "97   TB280FR  178502329        2022-06-24           NGY(WW)        ZEEBRUGGE   \n",
      "98   TB280FR  178502330        2022-06-27           NGY(WW)        ZEEBRUGGE   \n",
      "99   TB280FR  178502335        2022-07-01           NGY(WW)        ZEEBRUGGE   \n",
      "100  TB280FR  178502336        2022-07-04           NGY(WW)        ZEEBRUGGE   \n",
      "\n",
      "     qty Flags  UNPOL  UNPOD  \n",
      "0      1    []  JPNGO  USSAV  \n",
      "1      1    []  JPNGO  USSAV  \n",
      "2      1    []  JPNGO  USSAV  \n",
      "3      1    []  JPNGO  USSAV  \n",
      "4      1    []  JPNGO  USSAV  \n",
      "..   ...   ...    ...    ...  \n",
      "96     1    []  JPNGO  BEZEE  \n",
      "97     1    []  JPNGO  BEZEE  \n",
      "98     1    []  JPNGO  BEZEE  \n",
      "99     1    []  JPNGO  BEZEE  \n",
      "100    1    []  JPNGO  BEZEE  \n",
      "\n",
      "[101 rows x 9 columns]\n",
      "\n",
      "and the cols are:  Index(['model', 'unit', 'installation_date', 'destination_(pol)',\n",
      "       'note_(unloading)', 'qty', 'Flags', 'UNPOL', 'UNPOD'],\n",
      "      dtype='object')\n",
      "File saved:  2023-01-14 09:07:57.244440\n",
      "File saved:  2023-01-14 09:07:57.338677\n",
      "14-01-2023 09:07:57  End Main Function: \n",
      "Total time taken: 5.146129846572876 seconds\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    print(datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"), \" Start Main Function: \")\n",
    "    df = Main()\n",
    "    print(datetime.datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\"), \" End Main Function: \")\n",
    "    print(\"Total time taken: \" +str(time.time()-start_time) + \" seconds\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1669877760199
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# xl = pd.ExcelFile(\"Original File_Takeuchi_7月 RORO WWOcean_20220622.xlsx\")\n",
    "# xl.sheet_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install xeus-python\n",
    "# xl.parse(\"Sheet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.13"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
